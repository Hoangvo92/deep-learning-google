https://developers.google.com/machine-learning/crash-course/generalization/peril-of-overfitting

https://developers.google.com/machine-learning/crash-course/exercises

https://developers.google.com/machine-learning/crash-course/training-and-test-sets/playground-exercise

https://developers.google.com/machine-learning/crash-course/representation/feature-engineering

https://developers.google.com/machine-learning/crash-course/representation/programming-exercise

https://colab.research.google.com/notebooks/mlcc/feature_sets.ipynb?utm_source=mlcc&utm_campaign=colab-external&utm_medium=referral&utm_content=featuresets-colab&hl=en

https://developers.google.com/machine-learning/crash-course/feature-crosses/playground-exercises

https://colab.research.google.com/notebooks/mlcc/logistic_regression.ipynb?utm_source=mlcc&utm_campaign=colab-external&utm_medium=referral&utm_content=logisticregression-colab&hl=en#scrollTo=dWgTEYMddaA-



Switching from L2 to L1 regularization dramatically reduces the delta between test loss and training loss.
Switching from L2 to L1 regularization dampens all of the learned weights.
Increasing the L1 regularization rate generally dampens the learned weights; however, if the regularization rate goes too high, the model can't converge and losses are very high.
